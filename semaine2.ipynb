{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4829de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import gym\n",
    "from gym import Env, spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbabe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "794a43f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, env):\n",
    "        self.is_discrete = \\\n",
    "            type(env.action_space) == gym.spaces.discrete.Discrete\n",
    "        \n",
    "        if self.is_discrete:\n",
    "            self.action_size = env.action_space.n\n",
    "            print(\"Action size:\", self.action_size)\n",
    "        else:\n",
    "            self.action_low = env.action_space.low\n",
    "            self.action_high = env.action_space.high\n",
    "            self.action_shape = env.action_space.shape\n",
    "            print(\"Action range:\", self.action_low, self.action_high)\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        if self.is_discrete:\n",
    "            action = random.choice(range(self.action_size))\n",
    "        else:\n",
    "            action = np.random.uniform(self.action_low,\n",
    "                                       self.action_high,\n",
    "                                       self.action_shape)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ca364420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action size: 100\n"
     ]
    }
   ],
   "source": [
    "env = City()\n",
    "#env.reset(random_start = True, start_shape = (4, 4))\n",
    "\n",
    "\n",
    "agent = Agent(env)\n",
    "state = env.reset(random_start = True, start_shape = (4, 4))\n",
    "\n",
    "\n",
    "history=[]\n",
    "for _ in range(10):\n",
    "    action = agent.get_action(state)\n",
    "    state, reward = env.step(action)\n",
    "    history.append([state,reward])\n",
    "    env.render()\n",
    "    \n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16d71ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "########a toi de tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0511682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stable_baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "67528a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "da12bfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to ./PPO_2\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3369, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Valentin\\AppData\\Local\\Temp\\ipykernel_27008\\2342266375.py\", line 2, in <cell line: 2>\n",
      "    model.learn(total_timesteps=25)\n",
      "  File \"C:\\Users\\Valentin\\AppData\\Roaming\\Python\\Python39\\site-packages\\stable_baselines3\\ppo\\ppo.py\", line 310, in learn\n",
      "    return super().learn(\n",
      "  File \"C:\\Users\\Valentin\\AppData\\Roaming\\Python\\Python39\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py\", line 247, in learn\n",
      "    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)\n",
      "  File \"C:\\Users\\Valentin\\AppData\\Roaming\\Python\\Python39\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py\", line 175, in collect_rollouts\n",
      "    new_obs, rewards, dones, infos = env.step(clipped_actions)\n",
      "  File \"C:\\Users\\Valentin\\AppData\\Roaming\\Python\\Python39\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py\", line 162, in step\n",
      "    return self.step_wait()\n",
      "  File \"C:\\Users\\Valentin\\AppData\\Roaming\\Python\\Python39\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py\", line 43, in step_wait\n",
      "    obs, self.buf_rews[env_idx], self.buf_dones[env_idx], self.buf_infos[env_idx] = self.envs[env_idx].step(\n",
      "  File \"C:\\Users\\Valentin\\AppData\\Roaming\\Python\\Python39\\site-packages\\stable_baselines3\\common\\monitor.py\", line 90, in step\n",
      "    observation, reward, done, info = self.env.step(action)\n",
      "  File \"C:\\Users\\Valentin\\AppData\\Roaming\\Python\\Python39\\site-packages\\gym\\wrappers\\time_limit.py\", line 50, in step\n",
      "ValueError: not enough values to unpack (expected 5, got 4)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 1982, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\stack_data\\core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\stack_data\\core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\stack_data\\core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"D:\\Anaconda\\lib\\site-packages\\executing\\executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log='./')\n",
    "model.learn(total_timesteps=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4d0fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######################## environement a adapter pour fonctioner sur ce model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc495ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLUE = [255, 0, 0]\n",
    "#GREEN = [0, 255, 0]\n",
    "RED = [0, 0, 255]\n",
    "\n",
    "WHITE = [255, 255, 255]\n",
    "GREY = [240, 240, 240]\n",
    "#BLACK = [0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7cc241",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class City(Env):\n",
    "    def __init__(self):\n",
    "        super(City, self).__init__()\n",
    "        \n",
    "        self.observation_shape = 10, 10\n",
    "        self.observation_space = spaces.Box(low = 0, high = 1,\n",
    "            shape = self.observation_shape, dtype = np.uint8)\n",
    "        \n",
    "        self.canvas_shape = 700, 700, 3 # width, height, color (BGR)\n",
    "        self.canvas = np.ones(self.canvas_shape, dtype = np.uint8) * 0\n",
    "        \n",
    "        self.action_space = spaces.Discrete(self.observation_shape[0] * self.observation_shape[1],)\n",
    "        \n",
    "        self.map = np.ones(self.observation_shape, dtype = np.uint8) * 0\n",
    "        self.offices = []\n",
    "        self.houses = []\n",
    "        pass\n",
    "    \n",
    "    def reset(self, random_start = False, start_shape = (4, 4)):\n",
    "        self.map = np.ones(self.observation_shape, dtype = np.uint8) * 0\n",
    "        self.offices = []\n",
    "        self.houses = []\n",
    "        \n",
    "        if random_start :            \n",
    "            for y in range((self.observation_shape[1] - start_shape[1]) // 2, (self.observation_shape[1] + start_shape[1]) // 2):\n",
    "                for x in range((self.observation_shape[0] - start_shape[0]) // 2, (self.observation_shape[0] + start_shape[0]) // 2):\n",
    "                    self.map[y, x] = random.randrange(3)\n",
    "                    if self.map[y, x] == 1 : self.offices.append((x, y))\n",
    "                    elif self.map[y, x] == 2 : self.houses.append((x, y))\n",
    "        return [[1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1]]\n",
    "    \n",
    "    def __search_nearest_office(self, x, y):\n",
    "        return int(1)\n",
    "    \n",
    "    def step(self, action):\n",
    "        action_x, action_y = action % self.observation_shape[1], action // self.observation_shape[1]\n",
    "        \n",
    "        self.houses.append((action_x, action_y))\n",
    "        self.map[action_y, action_x] = 2\n",
    "        self.draw_elements_on_canvas()\n",
    "        \n",
    "        reward = self.__search_nearest_office(action_x, action_y)\n",
    "        reward = -reward + 5\n",
    "        \n",
    "        return [[1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1]], reward,False,{}\n",
    "    \n",
    "    def __draw_element_on_canvas(self, x, y, color):\n",
    "        observation_width, observation_height = self.observation_shape\n",
    "        canvas_width, canvas_height, _ = self.canvas_shape\n",
    "\n",
    "        drawing_width = int(canvas_width / observation_width)\n",
    "        drawing_height = int(canvas_height / observation_height)\n",
    "\n",
    "        # fit element to the canvas\n",
    "        for j in range(y * drawing_height, y * drawing_height + drawing_height):\n",
    "            for i in range(x * drawing_width, x * drawing_width + drawing_width):\n",
    "                try : self.canvas[i, j] = color\n",
    "                except IndexError : pass\n",
    "                \n",
    "        for j in range(y * drawing_height, y * drawing_height + drawing_height):\n",
    "            try : self.canvas[x * drawing_width, j] = GREY\n",
    "            except IndexError : pass\n",
    "            \n",
    "        for i in range(x * drawing_width, x * drawing_width + drawing_width):\n",
    "            try : self.canvas[i, y * drawing_height] = GREY\n",
    "            except IndexError : pass\n",
    "        pass\n",
    "\n",
    "    def draw_elements_on_canvas(self):\n",
    "        \n",
    "        # draw each element of the matrix\n",
    "        for y in range(len(self.map)):\n",
    "            for x in range(len(self.map[0])):\n",
    "                color = WHITE\n",
    "                if self.map[y, x] == 1 : color = BLUE # offices\n",
    "                elif self.map[y, x] == 2 : color = RED # houses\n",
    "                \n",
    "                self.__draw_element_on_canvas(x, y, color)\n",
    "        pass\n",
    "    \n",
    "    def render(self, mode = \"human\"):\n",
    "        if mode == \"human\" :\n",
    "            cv2.imshow(\"\", self.canvas)\n",
    "            cv2.waitKey(10)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1668b18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "   11/10000 [..............................] - ETA: 1:21:13 - reward: 4.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Valentin\\AppData\\Roaming\\Python\\Python39\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   50/10000 [..............................] - ETA: 1:26:02 - reward: 4.0000done, took 26.118 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d844b6f670>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "env = City()\n",
    "env.observation_space.sample()\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "\n",
    "def build_model(states, actions):\n",
    "    model = Sequential()    \n",
    "    model.add(Dense(24, activation='relu', input_shape=((1,10,10))))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model\n",
    "\n",
    "model = build_model(states, actions)\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=50, visualize=False, verbose=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "371a1395",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f9f934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66edba00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595bacb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086808e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc93720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f3e02d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac21ee5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc3ca52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6fabf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ade3577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af070bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f0a8d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b3743e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6414b408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3645de47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416fbe26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f079ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
